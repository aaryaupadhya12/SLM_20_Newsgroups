{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:12:56.862190Z",
     "iopub.status.busy": "2026-02-15T14:12:56.861919Z",
     "iopub.status.idle": "2026-02-15T14:14:56.294652Z",
     "shell.execute_reply": "2026-02-15T14:14:56.293995Z",
     "shell.execute_reply.started": "2026-02-15T14:12:56.862156Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-15 14:13:13.640120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771164793.825696      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771164793.889665      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771164794.374159      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771164794.374206      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771164794.374209      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771164794.374212      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9390965c9d0945a99cfce3668695c585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/734 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bed50689dd64c1a9b7de0734f8ad0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdc6226a3264ee0ab68c65197a0983a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/8.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8436054a104a08aad8d8e3d3e178bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/11314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3010da62740e4ad88a1fcccad35d16ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba4fda75968455ca053f21a3c8dea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef14f4e24e7c4b7e88272ce36689d49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10aad5a5b91c4564ae18bb95acebbd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97c3c28d3534f6cad6722e17e6e847f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a41a75c15b4571815b318e01f38dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11314 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8827856911f04cd79fdca5bf5d1379ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d050c035b7412db5c57f3812c1bd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "\n",
    "tokenized_20NG = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "train_encodings = tokenized_20NG['train']\n",
    "train_labels = torch.tensor(dataset[\"train\"][\"label\"])\n",
    "\n",
    "\n",
    "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "train_attention_mask = torch.tensor(train_encodings['attention_mask'])\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=20)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:19:54.498592Z",
     "iopub.status.busy": "2026-02-15T14:19:54.497824Z",
     "iopub.status.idle": "2026-02-15T14:50:23.227758Z",
     "shell.execute_reply": "2026-02-15T14:50:23.227071Z",
     "shell.execute_reply.started": "2026-02-15T14:19:54.498560Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up optimizer and scheduler...\n",
      "Total training steps: 2124\n",
      "Warmup steps: 212\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  14%|█▍        | 100/708 [01:26<08:45,  1.16it/s, loss=2.28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 100/708\n",
      "  Loss: 2.2818\n",
      "  Accuracy so far: 0.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  28%|██▊       | 200/708 [02:52<07:17,  1.16it/s, loss=1.85]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 200/708\n",
      "  Loss: 1.8490\n",
      "  Accuracy so far: 0.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  42%|████▏     | 300/708 [04:18<05:51,  1.16it/s, loss=1.74] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 300/708\n",
      "  Loss: 1.7421\n",
      "  Accuracy so far: 0.4838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  56%|█████▋    | 400/708 [05:44<04:25,  1.16it/s, loss=1.32] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 400/708\n",
      "  Loss: 1.3160\n",
      "  Accuracy so far: 0.5230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  71%|███████   | 500/708 [07:10<02:59,  1.16it/s, loss=1.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 500/708\n",
      "  Loss: 1.1189\n",
      "  Accuracy so far: 0.5531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  85%|████████▍ | 600/708 [08:36<01:33,  1.16it/s, loss=0.703]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 600/708\n",
      "  Loss: 0.7027\n",
      "  Accuracy so far: 0.5765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  99%|█████████▉| 700/708 [10:02<00:06,  1.16it/s, loss=0.751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 700/708\n",
      "  Loss: 0.7512\n",
      "  Accuracy so far: 0.5905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 708/708 [10:09<00:00,  1.16it/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/3 COMPLETED\n",
      "Average Loss:     1.5232\n",
      "Epoch Accuracy:   0.5917 (59.17%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  14%|█▍        | 100/708 [01:26<08:43,  1.16it/s, loss=1.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 100/708\n",
      "  Loss: 1.0349\n",
      "  Accuracy so far: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  28%|██▊       | 200/708 [02:52<07:17,  1.16it/s, loss=1.39] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 200/708\n",
      "  Loss: 1.3864\n",
      "  Accuracy so far: 0.7669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  42%|████▏     | 300/708 [04:18<05:51,  1.16it/s, loss=0.644]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 300/708\n",
      "  Loss: 0.6442\n",
      "  Accuracy so far: 0.7725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  56%|█████▋    | 400/708 [05:44<04:25,  1.16it/s, loss=0.762]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 400/708\n",
      "  Loss: 0.7617\n",
      "  Accuracy so far: 0.7681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  71%|███████   | 500/708 [07:10<02:59,  1.16it/s, loss=0.263]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 500/708\n",
      "  Loss: 0.2629\n",
      "  Accuracy so far: 0.7706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  85%|████████▍ | 600/708 [08:36<01:33,  1.16it/s, loss=1.12] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 600/708\n",
      "  Loss: 1.1166\n",
      "  Accuracy so far: 0.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  99%|█████████▉| 700/708 [10:03<00:06,  1.16it/s, loss=0.628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 700/708\n",
      "  Loss: 0.6277\n",
      "  Accuracy so far: 0.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 708/708 [10:09<00:00,  1.16it/s, loss=1.07] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2/3 COMPLETED\n",
      "Average Loss:     0.7764\n",
      "Epoch Accuracy:   0.7716 (77.16%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  14%|█▍        | 100/708 [01:26<08:43,  1.16it/s, loss=0.556]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 100/708\n",
      "  Loss: 0.5562\n",
      "  Accuracy so far: 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  28%|██▊       | 200/708 [02:52<07:18,  1.16it/s, loss=0.297]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 200/708\n",
      "  Loss: 0.2970\n",
      "  Accuracy so far: 0.8391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  42%|████▏     | 300/708 [04:18<05:51,  1.16it/s, loss=0.344]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 300/708\n",
      "  Loss: 0.3441\n",
      "  Accuracy so far: 0.8450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  56%|█████▋    | 400/708 [05:44<04:26,  1.16it/s, loss=0.398]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 400/708\n",
      "  Loss: 0.3983\n",
      "  Accuracy so far: 0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  71%|███████   | 500/708 [07:10<02:59,  1.16it/s, loss=0.294]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 500/708\n",
      "  Loss: 0.2935\n",
      "  Accuracy so far: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  85%|████████▍ | 600/708 [08:37<01:33,  1.15it/s, loss=0.506]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 600/708\n",
      "  Loss: 0.5055\n",
      "  Accuracy so far: 0.8451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  99%|█████████▉| 700/708 [10:03<00:06,  1.16it/s, loss=0.508]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Batch 700/708\n",
      "  Loss: 0.5084\n",
      "  Accuracy so far: 0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 708/708 [10:09<00:00,  1.16it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3/3 COMPLETED\n",
      "Average Loss:     0.5403\n",
      "Epoch Accuracy:   0.8452 (84.52%)\n",
      "Saving model and tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./bert-20newsgroups/tokenizer_config.json',\n",
       " './bert-20newsgroups/special_tokens_map.json',\n",
       " './bert-20newsgroups/vocab.txt',\n",
       " './bert-20newsgroups/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "loss_function = CrossEntropyLoss()\n",
    "\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {int(0.1 * total_steps)}\\n\")\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "        leave=True\n",
    "    )\n",
    "    \n",
    "    for batch_idx, (input_ids, attention_mask, labels) in progress_bar:\n",
    "        # Move to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function(logits, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Track loss and predictions\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Get predictions for accuracy calculation\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            current_acc = accuracy_score(all_true_labels, all_predictions)\n",
    "            print(f\"\\n  Batch {batch_idx+1}/{len(train_loader)}\")\n",
    "            print(f\"  Loss: {loss.item():.4f}\")\n",
    "            print(f\"  Accuracy so far: {current_acc:.4f}\")\n",
    "    \n",
    "    \n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    epoch_accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    \n",
    "    print(f\"EPOCH {epoch+1}/{epochs} COMPLETED\")\n",
    "    print(f\"Average Loss:     {average_loss:.4f}\")\n",
    "    print(f\"Epoch Accuracy:   {epoch_accuracy:.4f} ({epoch_accuracy*100:.2f}%)\")\n",
    "\n",
    "\n",
    "model.save_pretrained(\"./bert-20newsgroups\")\n",
    "tokenizer.save_pretrained(\"./bert-20newsgroups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:50:31.543553Z",
     "iopub.status.busy": "2026-02-15T14:50:31.542814Z",
     "iopub.status.idle": "2026-02-15T14:50:31.768239Z",
     "shell.execute_reply": "2026-02-15T14:50:31.767675Z",
     "shell.execute_reply.started": "2026-02-15T14:50:31.543512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T14:58:46.191226Z",
     "iopub.status.busy": "2026-02-15T14:58:46.190914Z",
     "iopub.status.idle": "2026-02-15T14:58:47.395348Z",
     "shell.execute_reply": "2026-02-15T14:58:47.394565Z",
     "shell.execute_reply.started": "2026-02-15T14:58:46.191200Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label names: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Example 1:\n",
      "  Text: 'I love playing basketball and football!'\n",
      "  Predicted: rec.sport.hockey\n",
      "  Confidence: 33.10%\n",
      "\n",
      "Example 2:\n",
      "  Text: 'This new graphics card is amazing for gaming'\n",
      "  Predicted: comp.sys.ibm.pc.hardware\n",
      "  Confidence: 31.77%\n",
      "\n",
      "Example 3:\n",
      "  Text: 'Jesus Christ is my savior and I believe in God'\n",
      "  Predicted: soc.religion.christian\n",
      "  Confidence: 74.78%\n",
      "\n",
      "Example 4:\n",
      "  Text: 'The stock market crashed today, all investors lost money'\n",
      "  Predicted: talk.politics.misc\n",
      "  Confidence: 62.55%\n",
      "\n",
      "Example 5:\n",
      "  Text: 'Python is the best programming language for machine learning'\n",
      "  Predicted: comp.graphics\n",
      "  Confidence: 46.60%\n",
      "\n",
      "Example 6:\n",
      "  Text: 'Medical Vaccines of COVID-19 has started Circulation'\n",
      "  Predicted: sci.med\n",
      "  Confidence: 89.46%\n",
      "\n",
      "================================================================================\n",
      "✓ INFERENCE COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"./bert-20newsgroups\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./bert-20newsgroups\")\n",
    "model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"Model loaded!\\n\")\n",
    "\n",
    "# Tokenize test data\n",
    "print(\"Tokenizing test data...\")\n",
    "test_encodings = tokenizer(\n",
    "    list(dataset['test']['text']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create PyTorch dataset and dataloader\n",
    "test_labels = torch.tensor(dataset['test']['label'])\n",
    "test_dataset = TensorDataset(\n",
    "    test_encodings['input_ids'],\n",
    "    test_encodings['attention_mask'],\n",
    "    test_labels\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(f\"Test set size: {len(test_labels)} samples\\n\")\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (input_ids, attention_mask, labels) in enumerate(test_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get predicted labels\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_true_labels.extend(labels.numpy())\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {(batch_idx + 1) * 32} / {len(test_labels)} samples\")\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "precision = precision_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy (weighted):  {accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted):    {recall:.4f}\")\n",
    "print(f\"F1-Score (weighted):  {f1:.4f}\")\n",
    "print()\n",
    "\n",
    "\n",
    "cm = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Confusion Matrix - 20 Newsgroups Test Set', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Confusion matrix saved as 'confusion_matrix.png'\\n\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def predict_text(text: str):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Get class & confidence\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    confidence = torch.softmax(logits, dim=1).max().item()\n",
    "    \n",
    "    # Get label name from dataset\n",
    "    sample = dataset['train'][0]  # Get one sample to see structure\n",
    "    # Use the predicted class as index in label_text\n",
    "    all_labels = set(dataset['train']['label_text'])\n",
    "    label_name = list(all_labels)[predicted_class]\n",
    "    \n",
    "    return label_name, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T15:11:19.631005Z",
     "iopub.status.busy": "2026-02-15T15:11:19.630189Z",
     "iopub.status.idle": "2026-02-15T15:11:19.645455Z",
     "shell.execute_reply": "2026-02-15T15:11:19.644666Z",
     "shell.execute_reply.started": "2026-02-15T15:11:19.630972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (weighted):  0.7144\n",
      "Precision (weighted): 0.7153\n",
      "Recall (weighted):    0.7144\n",
      "F1-Score (weighted):  0.7115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "precision = precision_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Accuracy (weighted):  {accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted):    {recall:.4f}\")\n",
    "print(f\"F1-Score (weighted):  {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
